{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c386295",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models import FastText\n",
    "import random\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a1a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import subprocess\n",
    "try:\n",
    "    nltk.data.find('wordnet.zip')\n",
    "except:\n",
    "    nltk.download('punkt', download_dir='/kaggle/working/')\n",
    "    nltk.download('wordnet', download_dir='/kaggle/working/')\n",
    "    nltk.download('stopwords', download_dir='/kaggle/working/')\n",
    "    nltk.download('wordnet', download_dir='/kaggle/working/')\n",
    "    nltk.download('averaged_perceptron_tagger', download_dir='/kaggle/working/')\n",
    "    \n",
    "    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n",
    "    subprocess.run(command.split())\n",
    "    nltk.data.path.append('/kaggle/working/')\n",
    "    \n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4551c106",
   "metadata": {},
   "source": [
    "# Reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c39f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv',\n",
    "                 encoding = 'latin',header=None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89aa512",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d9db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1706a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['id', 'date', 'query', 'user_id'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c587433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to map sentiment values\n",
    "sentiment_mapping = {0: \"Negative\", 4: \"Positive\"}\n",
    "\n",
    "# Map the 'sentiment' column using the defined dictionary\n",
    "df['sentiment'] = df['sentiment'].map(sentiment_mapping)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5479af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Count the occurrences of each sentiment category\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('Distribution of Sentiments')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5238b3b",
   "metadata": {},
   "source": [
    "# Taking a sample from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd7a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sample DataFrame with the same distribution of classes\n",
    "sample_size = 500000\n",
    "\n",
    "# Randomly sample 100000 rows from each class\n",
    "sampled_df = df.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(int(sample_size/2)))\n",
    "\n",
    "# Reset the index of the sampled DataFrame\n",
    "sampled_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Count the occurrences of each sentiment category in the sampled DataFrame\n",
    "sentiment_counts = sampled_df['sentiment'].value_counts()\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('Distribution of Sentiments in Sampled DataFrame')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7f98ad",
   "metadata": {},
   "source": [
    "# Applying Preprocessing on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f790a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_usernames(text):\n",
    "    # Remove usernames (mentions) from the text\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def normalize_tokens(tokens):\n",
    "    # Normalize tokens to lowercase\n",
    "    normalized_tokens = [token.lower() for token in tokens]\n",
    "    return normalized_tokens\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    # Filter out non-alphanumeric tokens\n",
    "    cleaned_tokens = [token for token in tokens if token.isalnum() and not token.isdigit()]\n",
    "    return cleaned_tokens\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    # Remove stop words from tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    # Lemmatize tokens based on part of speech\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        pos_tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "        pos_map = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV, \"J\": wordnet.ADJ}\n",
    "        wordnet_pos = pos_map.get(pos_tag, wordnet.NOUN)\n",
    "        lemma = lemmatizer.lemmatize(token, pos=wordnet_pos)\n",
    "        lemmatized_tokens.append(lemma)\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def filter_meaningful_tokens(tokens):\n",
    "    # Filter tokens based on length and presence in WordNet\n",
    "    meaningful_tokens = [token for token in tokens if len(token) >= 3]\n",
    "    return meaningful_tokens\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Step 1: Remove usernames\n",
    "    text = remove_usernames(text)\n",
    "     # Step 2: Tokenize text\n",
    "    tokens = tokenize_text(text)\n",
    "    \n",
    "    # Step 3: Normalize tokens\n",
    "    normalized_tokens = normalize_tokens(tokens)\n",
    "    \n",
    "    # Step 4: Clean tokens\n",
    "    cleaned_tokens = clean_tokens(normalized_tokens)\n",
    "    \n",
    "    # Step 5: Remove stop words\n",
    "    filtered_tokens = remove_stopwords(cleaned_tokens)\n",
    "    \n",
    "    # Step 6: Lemmatize tokens\n",
    "    lemmatized_tokens = lemmatize_tokens(filtered_tokens)\n",
    "    \n",
    "    # Step 7: Filter meaningful tokens\n",
    "    meaningful_tokens = filter_meaningful_tokens(lemmatized_tokens)\n",
    "    \n",
    "    # Join tokens into preprocessed text\n",
    "    preprocessed_text = ' '.join(meaningful_tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c919ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize tqdm with pandas apply function\n",
    "tqdm.pandas()\n",
    "\n",
    "# Assuming sampled_df is your DataFrame and preprocess_text is your preprocessing function\n",
    "sampled_df['cleaned_text'] = sampled_df['text'].progress_apply(preprocess_text)\n",
    "\n",
    "# Display the DataFrame with cleaned text\n",
    "print(sampled_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ab44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path and name for the CSV file\n",
    "csv_file_path = '/kaggle/working/sample_dataframe.csv'\n",
    "\n",
    "# Export the DataFrame to CSV\n",
    "sampled_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"DataFrame has been exported to {csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd43daff",
   "metadata": {},
   "source": [
    "# Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a30c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into features (X) and target (y)\n",
    "X = sampled_df['cleaned_text']\n",
    "y = sampled_df['sentiment']\n",
    "\n",
    "# Splitting the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y , random_state=42)\n",
    "\n",
    "# Convert sentiment labels to numerical format\n",
    "y_train_numeric = (y_train == 'Positive').astype(int)\n",
    "y_test_numeric = (y_test == 'Positive').astype(int)\n",
    "\n",
    "# Displaying the shapes of the new DataFrames\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c287c",
   "metadata": {},
   "source": [
    "# Usign FastText to get the embeddings of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b1d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'cleaned_text' from training data (X_train) to list of sentences\n",
    "train_sentences = [text.split() for text in X_train]\n",
    "\n",
    "# Train FastText model\n",
    "model = FastText(sentences=train_sentences, vector_size=30, window=2, min_count=1, sg=1, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672dbdbb",
   "metadata": {},
   "source": [
    "# Preparing the data before feeding it into the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beed514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pickle\n",
    "\n",
    "# Tokenize and convert text data to sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Save the tokenizer after fitting\n",
    "with open('tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Tokenizer saved to tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d688743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822410c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_len = 30\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_len)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding matrix from FastText model\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, 30))  # vector_size=30 in FastText\n",
    "for word, i in word_index.items():\n",
    "    if word in model.wv:\n",
    "        embedding_matrix[i] = model.wv[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b10836",
   "metadata": {},
   "source": [
    "# Model Preparing and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e1467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "# Build the simplified LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer using the embedding_matrix\n",
    "embedding_layer = Embedding(input_dim=vocab_size, \n",
    "                             output_dim=30, \n",
    "                             embeddings_initializer=Constant(embedding_matrix), \n",
    "                             input_length=max_len, \n",
    "                             trainable=True)  # Set trainable to True if you want to fine-tune the embeddings\n",
    "model.add(embedding_layer)\n",
    "\n",
    "# Single LSTM layer\n",
    "model.add(LSTM(50, dropout=0.2))  # Reduced units and dropout\n",
    "\n",
    "# Fully connected layer with fewer units\n",
    "model.add(Dense(8, activation='relu'))  # Reduced units\n",
    "# Output layer for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed97092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with numerical labels and store the training history\n",
    "history = model.fit(X_train_padded, y_train_numeric, epochs=5, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885aeea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f5ed15",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f595dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Predict classes on test data\n",
    "y_pred = (model.predict(X_test_padded) > 0.5).astype(int)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_numeric, y_pred)\n",
    "\n",
    "# Plot confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Positive'], \n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Display classification report\n",
    "report = classification_report(y_test_numeric, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d428a8",
   "metadata": {},
   "source": [
    "# Model Testing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd47284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure X_test and y_test_numeric are numpy arrays\n",
    "X_test_array = X_test.values if isinstance(X_test, pd.Series) else np.array(X_test)\n",
    "y_test_array = y_test_numeric.values if isinstance(y_test_numeric, pd.Series) else np.array(y_test_numeric)\n",
    "\n",
    "# Select 10 random indices\n",
    "random_indices = random.sample(range(len(X_test_array)), 10)\n",
    "\n",
    "# Prepare data for the table\n",
    "table_data = []\n",
    "for idx, rand_idx in enumerate(random_indices, start=1):\n",
    "    sentence = X_test_array[rand_idx]\n",
    "    true_label = \"Positive\" if y_test_array[rand_idx] == 1 else \"Negative\"\n",
    "    predicted_label = \"Positive\" if y_pred[rand_idx] == 1 else \"Negative\"\n",
    "    table_data.append([idx, sentence, true_label, predicted_label])\n",
    "\n",
    "# Create headers for the table\n",
    "headers = [\"Index\", \"Sentence\", \"True Label\", \"Predicted Label\"]\n",
    "\n",
    "# Display the table with 'fancy_grid' format\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f72a2b8",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6199dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in HDF5 format\n",
    "model.save('sentiment_analysis_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
